{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Introduction to Notebooks for SPSS Professionals - Part 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to help professionals who currently use SPSS Modeler understand how a data scientist can use notebooks for implementing various analytics use cases. \n",
    "\n",
    "We chose a simple analytics use case for this lab - predicting mortgage default. We are using different predictive models (C5 classification model in SPSS and Spark ML Random Forest Classifier in the notebook) because we want to focus on explaining functionality, and not comparing performance or accuracy.  \n",
    "\n",
    "\n",
    "## Notebooks\n",
    "\n",
    "This notebook IDE is built on top of open source Jupyter notebooks. A notebook is a file type that contains \n",
    "1. Code (Python, R or Scala)\n",
    "2. Markdown (comments like the ones you are reading now)\n",
    "3. A connection to a kernel (runtime environment). The kernel in this environment is provided by Spark. \n",
    "\n",
    "The content of a notebook is organized in \"cells\". The two main types of cells are \"code\" and \"markdown\". As you review the rest of the notebook, you may see some text that's marked with \"out\". The \"out\" tag is for output of code directly above it. If you don't see the out tag, then the output may have been cleared or the code in the cell didn't produce any printable output. \n",
    "\n",
    "While notebook IDE looks different from SPSS Modeler, these tools have several similarities on the technical level. When we run a Modeler stream, the visual nodes are converted to code which is executed in Modeler server. When we run notebooks, the code that we provide runs in the specific kernel for each programming language (the notebook IDE automatically starts a kernel for the programming language in which the notebook is implemented). \n",
    "\n",
    "You will notice that the notebook IDE has a menu that's dedicated to kernels with actions like Start, Restart, Reconnect, etc. This is similar to connections to Modeler Server. Notebook IDE offers more flexibility for connections to the kernel and the ability to switch kernels because several versions of programming languages are supported. \n",
    "\n",
    "When we work with notebooks, we have an option to run the entire notebook (all cells in the notebook) or individual cells. This is similar to running the entire Modeler stream or a just selected branch. If you are running individual cells, it's important that the cells above it had been run. Again, this is similar to Modeler (we don't start execution in the middle of the stream, and if we do, we turn on caching). \n",
    "\n",
    "If you want to run a cell, position the cursor at the end of the last line in the cell, and select menu Cell -> Run Cells (or click the Run icon). When the cell is running, you will see an  asterisks next to the cell [*]. Don't run any subsequent cells until execution is done.  \n",
    "\n",
    "\n",
    "Since Notebooks are based on an open source technlogy, you can find many tutorials and sample Notebooks. \n",
    "Here are some notebooks that show \"functional/technical\" features of Python and R: https://github.com/IBMDataScience/sample-notebooks\n",
    "You can find additional notebooks on the Community page of DSX.\n",
    "\n",
    "\n",
    "## Sample Modeler Stream\n",
    "\n",
    "Start with reviewing the sample Modeler stream that was provided by your instructor (MortgageDefault.str). In this lab we'll implement the same use case using Python and SparkML.\n",
    "\n",
    "![Mortgage Default SPSS stream](https://ibm.box.com/shared/static/hye9foibsh7wvexdfyp8h30pgpnc3cs3.jpg)\n",
    "\n",
    "## Working with a Notebook\n",
    "Until you click the Edit icon (pencil in the top right corner), you are looking at the \"static\" version of the notebook (i.e. it's simply displaying the content of the file, and it's not connected to a kernel). \n",
    "\n",
    "After you click the Edit icon, notice the \"Kernel starting\" message in the top right corner. You should also see a menu bar becuase we have opened the notebook IDE. \n",
    "\n",
    "Explore the menu options and let the instructor know if you have any questions. We'll use some of the menu options in this lab. \n",
    "\n",
    "As you are working on this lab, read the information in the markdown cells. To run code in each cell, position cursor at the end of the last line of code cell and click the Run icon. As mentioned earlier, the cell is still running if you see [*] next to the code cell, and not every cell will have printed output.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mortgage Default Use Case Implementation\n",
    "\n",
    "### Step 1: Connect to Object Storage\n",
    "\n",
    "We start with connecting to Object Storage. Object Storage is the Bluemix environment for storing flat files. \n",
    "\n",
    "You have loaded the data files as one of the first steps in the lab instructions, but the following code is connecting to one of the instructors' Object Storage. You can use this notebook without any changes. If you would like to load data files to your own Object Storage, please check with the lab instructor or see Appendix A of the lab instructions document.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_18c4556616c5444581b1cb6d212cf2dc(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', '879160f1a1174d2f912f196ac158ffbf')\n",
    "    hconf.set(prefix + '.username', 'aa5ea4cb9c48463681897f88b4a9ab08')\n",
    "    hconf.set(prefix + '.password', 'veCB_4)bYn362UU&')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', False)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_18c4556616c5444581b1cb6d212cf2dc(name)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The loan default information\n",
    "default = spark.read.format('csv')\\\n",
    "  .options(header='true', inferschema='true')\\\n",
    "  .load(\"swift://IntroToNotebooks.\" + name + \"/Default.csv\")\n",
    "\n",
    "# The property information\n",
    "property = spark.read.format('csv')\\\n",
    "  .options(header='true', inferschema='true')\\\n",
    "  .load(\"swift://IntroToNotebooks.\" + name + \"/Property.csv\")\n",
    "\n",
    "# Customer information\n",
    "customer = spark.read.format('csv')\\\n",
    "  .options(header='true', inferschema='true')\\\n",
    "  .load(\"swift://IntroToNotebooks.\" + name + \"/Customer.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Merge Files\n",
    "This step is similar to Merge node in Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = customer.join(property, customer['ID'] == property['ID'])\\\n",
    "                   .join(default, customer['ID']==default['ID']).select(customer['*'],property['SalePrice'], property['Location'], default['MortgageDefault'])\n",
    "# Preview  5 rows\n",
    "merged.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark SQL also allow you to use standard SQL\n",
    "customer.createOrReplaceTempView(\"customer\")\n",
    "property.createOrReplaceTempView(\"property\")\n",
    "default.createOrReplaceTempView(\"default\")\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "\n",
    "SELECT c.*\n",
    "    , p.SalePrice\n",
    "    , p.Location\n",
    "    , d.MortgageDefault\n",
    "FROM customer c\n",
    "INNER JOIN property p\n",
    "    on c.ID = p.ID\n",
    "INNER JOIN default d\n",
    "    on c.ID = d.ID\n",
    "    \n",
    "\"\"\"\n",
    "spark.sql(sql).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data understanding\n",
    "This capability is similar to graphboard in Modeler.\n",
    "\n",
    "PixieDust is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.\n",
    "\n",
    "More information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "AppliedOnline",
      "showLegend": "true",
      "stacked": "true",
      "staticFigure": "false",
      "valueFields": "ID,Income"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Try creating differnt graphs\n",
    "from pixiedust.display import *\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Rename some columns\n",
    "This step is not a requirement, it just makes some columns names simpler to type with no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = merged.withColumnRenamed(\"Yrs at Current Address\", \"YearCurrentAddress\").withColumnRenamed(\"Yrs with Current Employer\",\"YearsCurrentEmployer\")\\\n",
    "                .withColumnRenamed(\"Number of Cards\",\"NumberOfCards\").withColumnRenamed(\"Creditcard Debt\",\"CCDebt\").withColumnRenamed(\"Loan Amount\", \"LoanAmount\")\n",
    "merged.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build the Spark pipeline and the Random Forest model\n",
    "\"Pipeline\" is an API in SparkML that's used for building models.\n",
    "Additional information on SparkML: http://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "stringIndexer1 = StringIndexer(inputCol='AppliedOnline', outputCol='AppliedOnlineEncoded')\n",
    "stringIndexer2 = StringIndexer(inputCol='Residence',outputCol='ResidenceEncoded')\n",
    "stringIndexer3 = StringIndexer(inputCol='MortgageDefault', outputCol='label')\n",
    "\n",
    "# Instanciate the algorithm and define 2 instances of the model with different model parameters\n",
    "rf1=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=6)\n",
    "rf2=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5)\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"Income\", \"AppliedOnlineEncoded\", \"ResidenceEncoded\", \"YearCurrentAddress\", \"YearsCurrentEmployer\", \"NumberOfCards\", \\\n",
    "                                       \"CCDebt\", \"Loans\", \"LoanAmount\", \"SalePrice\", \"Location\"], outputCol=\"features\")\n",
    "\n",
    "pipeline1 = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, assembler, rf1])\n",
    "pipeline2 = Pipeline(stages=[stringIndexer1, stringIndexer2, stringIndexer3, assembler, rf2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = merged.randomSplit([80.0,20.0], seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build models\n",
    "model1 = pipeline1.fit(train)\n",
    "model2 = pipeline2.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 7: Score the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results1 = model1.transform(test)\n",
    "results2 = model2.transform(test)\n",
    "# This is a preview of 5 rows\n",
    "results1.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Model Analysis\n",
    "Compare the precision of the two models, this is similar to Analysis node in Modeler.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision model1 = {:.2f}.'.format(results1.filter(results1.label == results1.prediction).count() / float(results1.count()))\n",
    "print 'Precision model2 = {:.2f}.'.format(results2.filter(results2.label == results2.prediction).count() / float(results2.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Model Evaluation\n",
    "This step is similar to the Evaluation node in Modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve, Model 1 = {:.2f}.'.format(evaluator.evaluate(results1))\n",
    "print 'Area under ROC curve, Model 2 = {:.2f}.'.format(evaluator.evaluate(results2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Save Model\n",
    "Save model in Object Storage. \n",
    "\n",
    "A separate notebook has been created for \"batch scoring deployment\". This deployment notebook retrieves the model from object storage and applies it to a new dataset. The notebook can be scheduled to run via the Notebook scheduler (the clock icon on the menu bar) or through the deployment interface in IBM ML (currently in beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick the best model and save it\n",
    "# Overwrite any existing saved model in the specified path\n",
    "model1.write().overwrite().save(\"IntroToNotebooks.mortgageDefaultModel\")\n",
    "print\"Saved model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have finished Part 1 of the lab. Follow steps in the lab instructions document to upload and test the model scoring notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}